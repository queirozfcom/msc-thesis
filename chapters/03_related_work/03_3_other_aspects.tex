\section{Other Aspects}\label{section:other_aspects}

In this section, we will go over a couple of aspects we deem important inasmuch as they are \textit{model-agnostic} - these can be use no matter what approach one takes for predicting tags in a social tagging context.

\subsection{Data Representation}

Feature representation is an essential part of any kind of machine learning, because any kind of information (be it text, images, sound, etc) must be encoded as vectors so that models can be trained on them. In this subsection we cite a couple of approaches that have leveraged alternative feature representations for the task of predicting social tags.

\cite{han_etal_2010} suggest an interesting technique wherein they use concepts from \textit{transfer learning} \citep{pan_yang_2010} to train an embedding matrix $M$ on a training dataset. At test time, $M$ is used to project the test data into a lower dimensional vector space such that the correlations between the multiple labels are kept. Finally, a simple regularized linear regression model is used to train an independent classifier for each label.

The authors claim that their method outperforms similar and baseline methods on an image tagging dataset, as measured by ranked metrics such as Mean Average Precision (MAP) and Precision@$k$.

\cite{kataria_agarwal_2015} suggest an approach whereby resources (in this case, text documents) and tags are represented in the same shared subspace. More specifically, they train so-called \textit{translational embeddings} \citep{bordes_etal_2013} using a shallow neural network that forces the feature vectors to assume representations that minimize a loss function that represents the relationship between documents and tags (and optionally users).\footnote{One may argue that this is similar to training paragraph vectors (as in \cite{le_mikolov_2014}), wherein neural networks are used to force the learning of vector representations that minimize the relationship between paragraph vectors and words therein.}

They claim their method outperfoms baseline methods, when used as preprocessing step for multi-label classification problems, where the actual classifiers may be neural networks, SVMs or method based on Gaussian Processes.

% \subsection{Dimensionality Reduction}

% \textit{Dimensionality reduction}\footnote{See \cite{maaten_etal_2008} for a comprehensive review on the topic} refers to techniques that aim at reducing the sizes of matrices and tensor such that a more compact and economic representation is obtained, while keeping information loss at a minimum.

% Some common examples of techniques aimed at reducing the dimensionality of data are Principal Component Analysis (PCA) \cite{wold_etal_1987}, Singular Value Decomposition (SVD) and Autoencoders \citep{bengio_etal_2007}.

% \cite{song_etal_2008} Low Rank Matrix Factorization


\subsection{Clustering}

Clustering\footnote{See \cite{jain_2010} for a comprehensive overview and summary on data clustering.} refers to a type of unsupervised machine learning techniques whose objective is to group instances in order to extract common patterns and other similarities.

\cite{shen_etal_2009}\footnote{This method is explained in detail in section \ref{section:experiment_part_2}.} use $k$-medoids clustering to predict tags for text data. They first break up each individual document into segments by using the \textit{TextTiling}  procedure \citep{hearst_1994} and then cluster the segments back together. Finally, classification is done using SVM classifiers.

In a somewhat similar approach, \cite{nikolopoulos_etal_2009} use $k$-means with additional connectivity constraints to break up images into regions.\footnote{This is known as image segmentation \citep{haralick_shapiro_1985}.} Then, regions from multiple images are clustered together in order to find the general topics represented in the images. Using a labelled dataset with multiple tags for each image, they construct a derived dataset where each region cluster is assigned the most common tags for all the regions in that cluster. Finally, a simple multi-label SVM classifier is used for actual prediction. 

\cite{leginus_etal_2012} present a different take on clustering because, unlike the previous examples, they apply clustering not on the examples but on the labels. In other words, they cluster labels into label clusters, based on similarity. They use clustering techniques such as $k$-Means, Spectral $k$-Means and Mean Shift and report gains in efficiency and accuracy when using these representations, when compared to using the full data.



