\section{Resource-centered Methods}\label{section:resource_centered_methods}

In this section, we present a collection of  \textit{resource-centered} methods for tag prediction. They are so called because they leverage only resource-specific information in order to predict what tags should be assigned to a new, unlabelled resource. In other words, all prediction are of \textit{unpersonalized} nature.

We note that, although our problem domain only includes textual documents, we chose to also mention in this section approaches used in other domains, such as audio, video and images. This is because we are mostly interested in how these approaches work irrespective of the choice of features used.

\subsection{Association Rule Mining}

\ednote{yada yada introduction}

In one of the earlier papers on social tag prediction, Heymann et al. \citeyearpar{heymann_etal_2008} have applied \textbf{association rules} of the form $X \rightarrow Y$ (where $X$ and $Y$ are tagsets) in order to expand the set of tags given to a resource. Using techniques such as \textit{Market Basket Analysis}, the authors derive association rules of length 4 and below, using a certain level of support \footnote{A rule's \textit{support} equals the number of examples where both $X$ and $Y$ are present.} as threshold to remove overly noisy rules.

The authors report \citep{heymann_etal_2008} that a surprisingly high number of high quality rules can be found (such as those representing \textit{type-of} relationships and synonyms). Furthermore, the added tags help increase precision and recall for user queries, when the result sets are augmented to include documents tagged with those tags.

The authors also claim that using larger and larger rules would probably increase performance, but computational complexity quickly become prohibitive.

\footnote{Note that this method assumes that a resource already has some tags assigned to it. These are then used to predict another set of tags. This is sometimes called \textbf{tag-set expansion} in order to differentiate it from methods that do not make this assumption.}

Another approach involving association rules was put forward by Van Leeuwen and Puspitaningrum in \citeyear{vanleeuwen_puspitaningrum_2012}. They acknowledge the fact that gains in performance brought about by using larger rules come at a high cost in terms of processing time. They, however, suggest that a compromise can be achieved by choosing a carefully selected set of association rules, such that performance is increase at a lower cost.

Their approach works by using a compression mechanism to efficiently compute expanded tagsets for any given tagset. It computes the most suitable expanded tagsets ranked by support. \footnote{The support for a given tagset is simply the number of times that particular tagset was assigned to a resource in the system.}

\subsection{Content-based tag propagation}

Here we provide a basic overview of methods which, in one way or another, use the content-based similarity to propagate tags from labelled instances to unlabelled ones.

In the first approach, SORDO et al. \citeyearpar{sordo_etal_2007} have used both first-order (stylistic) and second-order (mood-based, extracted from the stylistic ones) features and a neighbours-based similarity measure to propagate labels from labelled audio pieces to unlabelled ones.

They reported good results for the approach, as measured by rank-based metrics such as Spearman's rank and Precision@$k$. They claim that ignoring tags with too few assignments improves results and that sometimes using more neighbors is beneficial, while sometimes it's harmful.

It should be noted here that this work was not run on a broad folksonomy, since all examples were annotated by a single person.

Another interesting example is that of \cite{moxley_etal_2008}. Their approach uses many feature \textit{modalities} to represent a resource (in this case, videos). In other words, they use multiple sources of information to build a feature vector, namely text information from the video transcripts, image information from video snapshots and concept information from external source.

They report good results using set-based performance metrics (slight variants of precision and recall). Furthermore, they claim that using an average of features built from multiple modalities helps suppress the effect of noisy information.

In \cite{guillaumin_etal_2009}, the authors propose a weighted neighbor approach where one can choose an arbitrary distance measure (i.e. Euclidean, Manhattan, etc) one wishes to use to measure similarity between resource representations. Then, the optimal weights for each resource are found via the optimization of a custom loss function that encodes the accuracy each individual tag prediction. 

In other words, the dataset is used to inform the decision on what weights to use for each resource. This will, in turn, define to what extent tag assignments for each resource will influence those of its neighbors.

This approach has been called \textit{metric learning} and, according to the authors, it has been used in the past in other contexts.

In \cite{li_etal_2009}, the authors have approached the problem from a slightly different angle. Although they have also used content-based similarity to search for neighbors, the weight given to each tag is not just proportional to the similarity between each pair of neighbors; it also incorporates a term that normalizes each tag according to the tag's \textit{prior}, i.e. the overall frequency of a given tag in the whole dataset.

By using rank-based metrics such as Precision@$k$ and Mean Average Precision (\textbf{MAP}), reporting that their method consistently outperforms methods that don't take a tag's overall prior into account.

In conclusion, two common themes in such \textit{content-based} tag propagation approaches seem to be \textbf{a)} designing similarity measures and other ways to retrieve similar resources given a \textit{query} resource and \textbf{b)} once the neighbor resources are found, find meaningful ways to weigh the contribution given by each neighbor in order to predict tags for the query resource. 

\subsection{Resource-based tag propagation}

In this subsection, we will talk about methods which use information \textit{about} the resource (other than its contents encoded as features) to build representations for these resources. These representations are then used in neighbor-based algorithms for actual classification.

\ednote{can I find some OTHER guy who just represented each resource as a vector over tags and did something simple with it?
}

\cite{auyeung_etal_2009} propose a slightly different approach. They encode each resource as a vector over the space of the full tag vocabulary, so that it resembles a bag-of-words approach, using tags instead of terms in the document. Similarity between resources is then calculated via simple measures like cosine similarity.

The authors report above-benchmark performance when using the described approach to predict tags for unlabelled examples. Metrics used to for gauging performance include Precision@$k$ and NDCG.


\subsection{Multi-label Classification/Ranking}

Since resources in an \textbf{STS} can be assigned multiple tags, it is natural to model this problem as a \textbf{Multi-label Classification} (MLC) problem.

Multi-label learning \footnote{Not to be confused with \textit{multi-class} classification.} (\cite{tsoumakas_katakis_2007}) refers to learning from data that is multi-labelled, that is, data where each example has not just a single label \footnote{These are called \textit{single-label} classification in MLC literature} but multiple ones.

A more particular approach, generally called \textit{Multi-label Ranking}, refers (\cite{illig_etal_2011}) to problems where not only do instances have multiple labels associated with them, but every label also has a \textit{rank}; in other words, each label assignment also carries a weight, so that labels assigned to a particular example may be ranked with respect to the weight each label has. This is in contrast with regular multi-label classification, where labels are represented with a binary vector, making no distinction between labels. \footnote{Although our own method is of the multi-label ranking type, we find it worthwhile to list regular MLC method due to how similar both are.}

\ednote{ADD example here with an instance with binary labels and another one with ranked labels}.

In \cite{katakis_etal_2008}, the authors have applied multi-label classification to the task of classifying HTML pages and journal abstracts into tags.

The chosen method was to train a binary classifier for each individual label, a meta-classification procedure called \textit{Binary Relevance} \footnote{\textit{Binary Relevance} is an adaptation of the well-known \textit{One-versus-All} \citep{rifkin_klautau_2004} classifier, commonly used for multi-class classification.} in the MLC community. The underlying classifier was a simple Na√Øve Bayes model, trained on the bag-of-words representation of the text documents.

The authors claim good results with their model, while noting that they have restricted the tag vocabulary to those tags appearing in at least 50 documents in order to trim rare tags.

\cite{bertin-mahieux_etal_2008} use a 2-level model to predict tags for audio pieces from a popular STS for songs, namely \textit{last.fm} \footnote{Reachable online via http://last.fm}.

For the first level, they use a technique called \citet{Filter Boosting}, which is an extension to \textit{Adaptive Boosting} meta-learning, better suited at online learning with large datasets. Using decision stumps (decision trees with a single level) as the underlying classifier, they train an individual classifier for each tag, which is also used to extract features to be used downstream.

The second level is another Filter Boosting classifier trained on the output of the first one, possibly dropping features found to be irrelevant by the first level.

They reportedly beat previous performances on this particular dataset and noted that using the first level for feature selection seems to help with generalization.

\cite{shen_etal_2009} introduce a different approach to predicting tags, namely one leveraging \textit{multi-instance} learning \citep{dietterich_etal_1997}, whereby one considers a single training example as a \textit{bag} of instances, rather than a single entity.

The technique \footnote{This technique was adapted from a previous work by \cite{zhou_zhang_2006} in scene classification} combines multi-instance learning with multi-label learning by splitting a single resource (in this case, tagged documents from the \textit{Delicious} website) into a bag of individual parts, combining these into a single instance by means of clustering and then using those for classifying the original resource into multiple tags.

More specifically, they use a well-known text segmentation algorithm called \textit{TextTiling} \citep{hearst_1994} to split each document into segments. This turns each document into a bag of segments. Then, as per the technique, each bag of segments is transformed into a single feature vector, by means of \textit{k-medoids} clustering \citep{kaufmanl_rousseeuw_1987}.\footnote{In order to enable clustering of multiple bags of vectors, a custom distance metric needs to be used. In this case, the  \textit{Hausdorff} distance metric \citep{huttenlocher_etal_1993} was used.} Once the problem has been reduced into a regular multi-label ranking problem, a simple \textit{one-vs-all} metaclassifier using an SVM model is used for actually predicting  tag scores.

The authors report that this method compares favourably against other common multi-label models such as Binary Relevance and ML-$k$-NN \citep{zhang_zhou_2007}, as evaluated by metrics such as Precision @k, Recall @k and Accuracy @k.

\cite{illig_etal_2011}

ok

\cite{song_etal_2011} is an interesting work inasmuch as almost equal attention is given to performance and to training time. They train an adapted \textit{Gaussian Process} model on three different datasets with varying characteristics (\textit{Delicious, Bibsonomy and CiteULike}). They argue that Gaussian Processes are a good fit to the problem at hand (label ranking) because they naturally output posterior probabilities for each class, which can be naturally used for label ranking.

In order to make training and inference faster, they only choose $M$, where $M <<< N$ to estimate the hyperparameters for the model, yielding significant gains in training and test times.

Notably, the authors report that their method outperforms competitive alternatives such as SVM by as much as 30\% while only using 5\% of the training data (due to the selection of prototypes).

\cite{kataria_agarwal_2015}
Words, documents, tags and users are embedded in a shared subspace.
They start off with the assumption that using tags (in addition to words) to learn document representations can help with regularization, because it adds ‚Äútopic-level‚Äù information to the document representations.
In content-based recommendation, they learn representations for documents and rank the tags for a resource using KNN, SVM Struct and Sparse Gaussian Processes.
Two-stage method:
Stage 1: models one layer using the tags for a document and another layer using the words for that document.
Stage 2: use the representations learned in stage 1 to learn user representations, using ‚Äúembedding translation‚Äù to find the ‚Äúuser‚Äù representation that is closest to ‚Äúdocument‚Äù + ‚Äútag‚Äù

\cite{tao_yao_2016}

Framed the problem as a multi-label text classification problem. They extract features using TFIDF and also Doc2Vec. For ranking tags, they use Neural Nets (binary sigmoid outputs) and SVMs.


\subsection{Matrix Factorization Methods}

\cite{si_sun_2008} Tag-LDA (LDA on Resource-content-tags)

\cite{bundschus_etal_2009} LDA on the resource-tag matrix.

\cite{panagakis_kotropoulos_2011} PARAFAC2 tensor factorization.

\cite{hu_etal_2012} Train an LDA model on documents -> topic distr, words -> topic distr, tags -> topic distr.

\cite{zhang_etal_2014} Apply a Sparse Version of LSA on the resource-tag matrix using CUDA GPU.


\subsection{Graph-based}

\cite{sharma_bedi_2009} Ant-colony approach, with parameterized evaporation.

\cite{wang_etal_2015} Develop a method focused on "hesitant" (i.e. sparsely tagged) webpages. Resources are clustered into rough sets using DenShrink method, to find resource neighbours.

They use graph concepts like Hubs, Bridges and Attached Nodes.

\subsection{Other}

\cite{si_sun_2010} Probabilistic.

\cite{trabelsi_etal_2012} Probabilistic approach (Hidden-Markov Model (HMM)). They train HMMs to model the sequences of users' latent (hidden) "intents" when tagging a resource, but the approach is not personalized at inference time.

\cite{liu_etal_2013} Combine 3 methods linearly: TFIDF keyword extraction, Item-based CF and Corr-LDA.



\cite{sattigeri_etal_2014}
Deep Convolutional nets for learning good features for audio tagging. Uses sparse coding, topic models.
Their contribution is to suggest a custom pooling mechanism for convoluted features.
For classification, they use things like weighted voting ensembles and Linear SVM.


