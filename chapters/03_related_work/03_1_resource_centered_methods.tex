\section{Resource-centered Methods}\label{section:resource_centered_methods}

In this section, we present a collection of  \textit{resource-centered} methods for tag prediction. They are so called because they leverage only resource-specific information in order to predict what tags should be assigned to a new, unlabelled resource. In other words, all prediction are of \textit{unpersonalized} nature.

We note that, although our problem domain only includes textual documents, we chose to also mention in this section approaches used in other domains, such as audio, video and images. This is because we are mostly interested in how these approaches work irrespective of the choice of features used.

\subsection{Association Rule Mining}

\ednote{yada yada introduction}

In one of the earlier papers on social tag prediction, Heymann et al. \citeyearpar{heymann_etal_2008} have applied \textbf{association rules} of the form $X \rightarrow Y$ (where $X$ and $Y$ are tagsets) in order to expand the set of tags given to a resource. Using techniques such as \textit{Market Basket Analysis}, the authors derive association rules of length 4 and below, using a certain level of support \footnote{A rule's \textit{support} equals the number of examples where both $X$ and $Y$ are present.} as threshold to remove overly noisy rules.

The authors report \citep{heymann_etal_2008} that a surprisingly high number of high quality rules can be found (such as those representing \textit{type-of} relationships and synonyms). Furthermore, the added tags help increase precision and recall for user queries, when the result sets are augmented to include documents tagged with those tags.

The authors also claim that using larger and larger rules would probably increase performance, but computational complexity quickly become prohibitive.

\footnote{Note that this method assumes that a resource already has some tags assigned to it. These are then used to predict another set of tags. This is sometimes called \textbf{tag-set expansion} in order to differentiate it from methods that do not make this assumption.}

Another approach involving association rules was put forward by Van Leeuwen and Puspitaningrum in \citeyear{vanleeuwen_puspitaningrum_2012}. They acknowledge the fact that gains in performance brought about by using larger rules come at a high cost in terms of processing time. They, however, suggest that a compromise can be achieved by choosing a carefully selected set of association rules, such that performance is increase at a lower cost.

Their approach works by using a compression mechanism to efficiently compute expanded tagsets for any given tagset. It computes the most suitable expanded tagsets ranked by support. \footnote{The support for a given tagset is simply the number of times that particular tagset was assigned to a resource in the system.}

\subsection{Content-based tag propagation}

Here we provide a basic overview of methods which, in one way or another, use the content-based similarity to propagate tags from labelled instances to unlabelled ones.

In the first approach, SORDO et al. \citeyearpar{sordo_etal_2007} have used both first-order (stylistic) and second-order (mood-based, extracted from the stylistic ones) features and a neighbours-based similarity measure to propagate labels from labelled audio pieces to unlabelled ones.

They reported good results for the approach, as measured by rank-based metrics such as Spearman's rank and Precision@$k$. They claim that ignoring tags with too few assignments improves results and that sometimes using more neighbors is beneficial, while sometimes it's harmful.

It should be noted here that this work was not run on a broad folksonomy, since all examples were annotated by a single person.

Another interesting example is that of \cite{moxley_etal_2008}. Their approach uses many feature \textit{modalities} to represent a resource (in this case, videos). In other words, they use multiple sources of information to build a feature vector, namely text information from the video transcripts, image information from video snapshots and concept information from external source.

They report good results using set-based performance metrics (slight variants of precision and recall). Furthermore, they claim that using an average of features built from multiple modalities helps suppress the effect of noisy information.

In \cite{guillaumin_etal_2009}, the authors propose a weighted neighbor approach where one can choose an arbitrary distance measure (i.e. Euclidean, Manhattan, etc) one wishes to use to measure similarity between resource representations. Then, the optimal weights for each resource are found via the optimization of a custom loss function that encodes the accuracy each individual tag prediction. 

In other words, the dataset is used to inform the decision on what weights to use for each resource. This will, in turn, define to what extent tag assignments for each resource will influence those of its neighbors.

This approach has been called \textit{metric learning} and, according to the authors, it has been used in the past in other contexts.

In \cite{li_etal_2009}, the authors have approached the problem from a slightly different angle. Although they have also used content-based similarity to search for neighbors, the weight given to each tag is not just proportional to the similarity between each pair of neighbors; it also incorporates a term that normalizes each tag according to the tag's \textit{prior}, i.e. the overall frequency of a given tag in the whole dataset.

By using rank-based metrics such as Precision@$k$ and Mean Average Precision (\textbf{MAP}), reporting that their method consistently outperforms methods that don't take a tag's overall prior into account.

In conclusion, two common themes in such \textit{content-based} tag propagation approaches seem to be \textbf{a)} designing similarity measures and other ways to retrieve similar resources given a \textit{query} resource and \textbf{b)} once the neighbor resources are found, find meaningful ways to weigh the contribution given by each neighbor in order to predict tags for the query resource. 

\subsection{Resource-based tag propagation}

In this subsection, we will talk about methods which use information \textit{about} the resource (other than its contents encoded as features) to build representations for these resources. These representations are then used in neighbor-based algorithms for actual classification.

\ednote{can I find some OTHER guy who just represented each resource as a vector over tags and did something simple with it?
}

\cite{auyeung_etal_2009} propose a slightly different approach. They encode each resource as a vector over the space of the full tag vocabulary, so that it resembles a bag-of-words approach, using tags instead of terms in the document. Similarity between resources is then calculated via simple measures like cosine similarity.

The authors report above-benchmark performance when using the described approach to predict tags for unlabelled examples. Metrics used to for gauging performance include Precision@$k$ and NDCG.


\subsection{Multi-label Classification/Ranking}

\cite{chen_etal_2008} OvR SVM

\cite{katakis_etal_2008} OvR SVM

\cite{bertin-mahieux_etal_2008} Boosted Trees (2-stage)

\cite{song_etal_2008}

\cite{shen_etal_2009} Multi-instance Multi-label

\cite{illig_etal_2011}

\cite{lo_etal_2011} SVM and adaboost

\cite{song_etal_2011} 
Prototype-based method. Trains a Gaussian Process (GP) classifier for each tag. Then a representative document (prototype) is found for each class. This is used to infer categories for each resource. Then tags are associated with categories and at inference time, tags are suggested depending upon the category of the query resource.

\cite{belem_etal_2014}
Object-centered approach. They extract all sorts of heuristics (tag co-occurrence, tags assigned to the object by other users, textual features from title, metadata and comments, tag stability, descriptive power, etc) to build the features for a resource and for tags.
For actual ranking of tag relevance, they use L2R: RankSVM, Random Forests and Genetic Programming approaches.

\cite{lo_etal_2014}
Multi-label classification approach. They create a variant of RAkEL where each underlying classifier is treated as an information source, but the weights (coefficients) given to each underlying classifier is not uniform, but learned.
Tag counts are treated are "costs" to minimize, too.

\cite{yagnik_etal_2014} 
Multi-label text classification approach, with naive bayes (multinomial) classifier (OvR).Focus is on new resources (no tags yet)

\cite{charte_etal_2015}

Framed as a multi-label text classification. Uses a multi-label KNN.

\cite{kataria_agarwal_2015}
Words, documents, tags and users are embedded in a shared subspace.
They start off with the assumption that using tags (in addition to words) to learn document representations can help with regularization, because it adds “topic-level” information to the document representations.
In content-based recommendation, they learn representations for documents and rank the tags for a resource using KNN, SVM Struct and Sparse Gaussian Processes.
Two-stage method:
Stage 1: models one layer using the tags for a document and another layer using the words for that document.
Stage 2: use the representations learned in stage 1 to learn user representations, using “embedding translation” to find the “user” representation that is closest to “document” + “tag”

\cite{tao_yao_2016}

Framed the problem as a multi-label text classification problem. They extract features using TFIDF and also Doc2Vec. For ranking tags, they use Neural Nets (binary sigmoid outputs) and SVMs.

\cite{barezi_etal_2017}
Multi-label classification approach. They cluster labels in groups where a label in one group does not has dependencies on tags in other groups, so they can be learned in parallel.

\subsection{Matrix Factorization Methods}

\cite{si_sun_2008} Tag-LDA (LDA on Resource-content-tags)

\cite{bundschus_etal_2009} LDA on the resource-tag matrix.

\cite{panagakis_kotropoulos_2011} PARAFAC2 tensor factorization.

\cite{hu_etal_2012} Train an LDA model on documents -> topic distr, words -> topic distr, tags -> topic distr.

\cite{zhang_etal_2014} Apply a Sparse Version of LSA on the resource-tag matrix using CUDA GPU.


\subsection{Graph-based}

\cite{sharma_bedi_2009} Ant-colony approach, with parameterized evaporation.

\cite{wang_etal_2015} Develop a method focused on "hesitant" (i.e. sparsely tagged) webpages. Resources are clustered into rough sets using DenShrink method, to find resource neighbours.

They use graph concepts like Hubs, Bridges and Attached Nodes.

\subsection{Other}

\cite{si_sun_2010} Probabilistic.

\cite{trabelsi_etal_2012} Probabilistic approach (Hidden-Markov Model (HMM)). They train HMMs to model the sequences of users' latent (hidden) "intents" when tagging a resource, but the approach is not personalized at inference time.

\cite{liu_etal_2013} Combine 3 methods linearly: TFIDF keyword extraction, Item-based CF and Corr-LDA.



\cite{sattigeri_etal_2014}
Deep Convolutional nets for learning good features for audio tagging. Uses sparse coding, topic models.
Their contribution is to suggest a custom pooling mechanism for convoluted features.
For classification, they use things like weighted voting ensembles and Linear SVM.


