\chapter{Proposal}\label{chap:proposal}

Initially, our proposal was to analyze multiple multi-label classification algorithms and verify how they perform when applied to the task of predicting social tags in broad folksonomies. We chose multi-label classification techniques because they are by far the most common method applied to social tag prediction (as evidenced in our literature review) and also because there is already a large body of work dedicated to this particular form of machine learning\footnote{See \cite{tsoumakas_katakis_2007} for a comprehensive overview of the subject.}

However, after reading many articles where this particular type of technique is applied to social tag prediction, we noticed that most use, in fact, a \textit{label ranking} approach, which is very similar to multi-label classification, but where continuous, rather than real-valued, scores are assigned to each label.\footnote{More about the relationship between multi-label classification and label ranking on section \ref{subsection:classification_vs_ranking}}

This has caused us to slightly modify our proposal; we changed our focus from multi-label classification to multi-label ranking. This has enabled us to compare methods that are actually in use in the literature and see how these results fare in comparison to those reported by other authors.

Our final proposal is twofold:

\textbf{Firstly}, we would like to verify the performance of a group of social tag prediction methods. We will use techniques that are widely used 


Additionally, we intend test these on two different datasets, which differ on key metrics such as average number of tags per resource, total number of tags, total number of resources, etc. This will enable us to compare the performance of these methods under different settings. If we had used a single dataset to experiment with, we could risk choosing one that unfairly benefits one method to the detriment of others. In other words, this is a way to reduce any possible bias that could result from using a single dataset to compare these methods.

We consider this an important issue because the tag vocabulary and the folksonomy as a whole exhibits \textit{emergent semantics} \citep{cattuto_etal_2007,koerner_etal_2010} due to its collaborative nature. This means that the characteristics of such systems may vary in multiple, sometimes unpredictable ways.

Among the characteristics datasets may differ in, we can count:

\begin{itemize}
    \item \textbf{Total number of resources}: The total number of resources in a dataset may affect the outcome of many prediction approaches, particularly those that need many samples to learn from.
    
    \item \textbf{Total number of unique tags}: A dataset where resources are tagged using a limited tag vocabulary will probably be more amenable to tag prediction, independently of the approach used.
    
    \item \textbf{Average number of tags per resource}: We suspect that the number of tags each resource has been assigned will have an impact on classification and ranking. This is because it may be easier to return valid tags if there are more to choose from (for a given resource).
    
    \item \textbf{Minimum and maximum number of tags per resource}: The fact that some datasets allow some resources to have either zero or an unlimited number of tags may affect the performance of ranking approaches that rely on some sort of calculated \textit{threshold} or cut-off value to define which tags are predicted.
    
    \item \textbf{Number of resources per tag}: The number of times each individual tag was assigned will probably be important because if there are too few examples some approaches may be unfeasible.

\end{itemize}

\textbf{Secondly}, we would like to verify to what extent the technique introduced by \cite{shen_etal_2009}, namely \textit{Multi-Instance Multi-label Learning for Automatic Tag Recommendation} works when applied to other kinds of textual features other than TF-IDF-weighted bag of words.

We propose this experiment because there are multiple techniques (mostly linear methods, such as Logistic Regression and SVM with a linear Kernel) that work well with bag of words due to their sparse nature \citep{hsu_etal_2010, li_etal_2015}, but may struggle with text representations where each document is represented not by a sparse feature vector but by a dense one instead.

We would therefore like to investigate if and in what way the results obtained using multi-instance learning for sparse vectors extrapolate for dense and otherwise different text representations.

One way to find that out is to try the aforementioned method with other representations for documents that have been used in the literature, which turn documents into \textit{dense} feature vectors, as follows:

\begin{itemize}
    \item \textbf{LDA Topic Probabilities}: As suggested in the original article that introduced LDA \citep{blei_etal_2003}, one can use topic probabilities for each topic as a \textit{representation} for a document. This is in spite of the fact that LDA is mainly a non-supervised technique to extract topic densities from a text corpus.
    
    \item \textbf{IDF-weighted Average of Word Embeddings}: Word embeddings\footnote{One of the seminal articles for word embeddings is \cite{bengio_etal_2003}.} are fixed-dimension, dense representations for individual words. It has been recently suggested that one used the IDF-weighted average of word embeddings in a document as a representation for that document \citep{zhao_etal_2015,correa_etal_2017}. Furthermore, this strategy has been established to work reasonably well according to many authors \citep{wieting_etal_2016,arora_etal_2017}.
    
\end{itemize}

For the same reasons as in proposal 1, we will conduct these experiments on two datasets with different characteristics, to avoid biased results.

