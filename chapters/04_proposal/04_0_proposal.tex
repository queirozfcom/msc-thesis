\chapter{Proposal}\label{chap:proposal}

Our proposal is twofold:

\textbf{Firstly}, we would like to verify the performance of a group of tag prediction methods on two different datasets, which differ on key metrics such as average number of tags per resource, total number of tags, total number of resources, etc.

This will provide insight into whether (if at all)  these techniques display difference in performance with respect with the characteristics of the datasets they are applied onto.

\textbf{Secondly}, we would like to verify to what extent the technique introduced by \cite{shen_etal_2009}, namely \textit{Multi-Instance Multi-label Learning for Automatic Tag Recommendation} works when applied to kinds of textual features other than TF-IDF-weighted bag of words.

We propose this experiment because there are multiple techniques (mostly linear methods, such as Logistic Regression and SVM with a linear Kernel) that work well with bag of words due to their sparse nature \cite{hsu_etal_2010, li_etal_2015}, but may struggle with denser text representations.

We would therefore like to investigate if and in what way the results obtained using multi-instance learning for sparse features extrapolate for dense and otherwise different text representations.

One way to find that out is to try the aforementioned method with two other other representations for documents that have been used in the literature, which turn documents into \textbf{dense} feature vectors, namely \textbf{a)} \textit{ParagraphVector} (introduced by \cite{le_mikolov_2014} \footnote{popularized as \textit{Doc2Vec} in the popular framework \textit{Gensim}} and \textbf{b)} using the IDF-weighted average of word embeddings in a document as a representation for that document, as seen in \cite{zhao_etal_2015,correa_etal_2017}, among others.